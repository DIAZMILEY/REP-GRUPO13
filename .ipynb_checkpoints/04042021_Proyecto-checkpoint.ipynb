{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DIAZMILEY/REP-GRUPO13/blob/main/04042021_Proyecto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqrpzgOXYGrv"
   },
   "source": [
    "# <center> **DOCUMENTACIÓN PROYECTO \"OCIO Y SENTIMIENTO\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfIyBOoleLu6"
   },
   "source": [
    "## ***INTRODUCCIÓN***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkeHzSzm1pQU"
   },
   "source": [
    "El proyecto tiene como fin realizar el analisis del sentimiento de los usuarios de Amazon especificamente en uno de sus productos llamado Amazon Prime Video, el estudio es realizará haciendo uso de la herramienta para recabar información usada por Amazon llamada IMBD, esta página permite que los usuarios califiquen los aspectos técnicos y narrativos de un filme creando una fuente de información alimentada por miles de personas.\n",
    "\n",
    "De acuerdo a lo anterior se pudo tener acceso a dos bases de datos: una llamada \"amazon instant video\" y la segunda \"movies and tv\", estas dos bases de datos tienen alrededor de 537.000 reviews, basados en la opinion de los usuarios se realiza una segmentación de la información se realiza una segmentación de las opiniones y las palabras clave obtenidas en cada comentario, de tal forma que se pueda posteriormente basado en las palabras utilizadas dar una califiación total al texto de tal forma que se pueda definir si la apreciación del usuario es positiva, neutra o negativa.  \n",
    "\n",
    "Las reviews en general están acompañadas de una calificación por lo que se podría deducir que tal review contiene implícitamente el valor de la calificación, implicando esto cierta complejidad, por eso se requiere el análisis de gran cantidad de palabras para lograr llegar al punto de predecir la posible calificación que dará una persona a un filme, se requiere el análisis de gran cantidad de datos, para crear una consistencia estadística que permita cierto grado precisión, basado en premisas lógicas como base de aprendizaje del programa.\n",
    "\n",
    "La información se obtendrá de la: [Base de Datos](https://jmcauley.ucsd.edu/data/amazon/). la cual contiene un amplio abanico de elección de bases de datos basado en los productos que ofrece AMAZON por lo que podría ser una buena fuente siendo este el mayor distribuidor de bienes del mundo contiene una base de datos consistente con gran cantidad y calidad. Esta base de datos esta delimitada por la fecha, para el caso de las bases a trabajar tienen información capturada entre 1997 al año 2014.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbMZdPsl5Tth"
   },
   "source": [
    "## ***`Amazon`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XigbRcs3RL1"
   },
   "source": [
    "Es una compañía Estadounidense fundada en 1994, la cual se dedica al comercio electrónico que va desde la venta de artículos en línea hasta la oferta de servicios en la nube. \n",
    "En este proyecto nos concentramos analizar la percepción de uno de sus nichos que es Amazon Prime Video, es un servicio que tiene disponibilidad en más de 200 países y su finalidad es poner a disposición de sus suscriptores películas y series, plataforma creada y gestionada por Amazon, esto la convierte en una competidora de Netflix y HBO.\n",
    "Una de las principales características de Amazon Prime Video es que su contenido está compuesto principalmente por películas y series producidas por Amazon, lo cual hace que tenga una base de datos reducida respecto de su competencia, a raíz de esto la empresa Amazon decidió aliarse con grandes compañías como Marvel, Pixar , Disney entre otras para transmitir todas sus películas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RpOuwXUeLzO"
   },
   "source": [
    "## ***ALCANCE***\n",
    "\n",
    "Va desde la identificación de bases de datos, su lectura y a partir de ella, realizar un analisis completo de los datos hasta la clasificación de los comentarios de los usuarios de Amazon Prime Video, de tal forma que se pueda calificar el servicio entre bueno o malo, hasta la creación de un listado de palabras clave que definan y confirmen si la opinion de un usuario es postivia o negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyH1O6d0eL4_"
   },
   "source": [
    "## ***OBJETIVOS***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy__suBxsyix"
   },
   "source": [
    "## ***`GENERAL`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijrkHVZE6l02"
   },
   "source": [
    "Elaborar un algoritmo de procesamiento de lenguaje natural, que permita identificar el sentimiento manifestado por los usuarios de Amazon Prime Video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soGaJWCasyl8"
   },
   "source": [
    "## ***`ESPECIFICOS`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsOhX84U623x"
   },
   "source": [
    "* Implementar un algoritmo que permita clasificar los sentimientos de los usuarios.\n",
    "\n",
    "* Realizar un análisis de todos los textos, que permita encontrar las palabras más relevantes o frecuentes, que influyen en la definición del sentimiento hacia el servicio utilizado.\n",
    "\n",
    "* Generar clústeres de palabras que permitan definir un constructo en función de los sentimientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9S0T8T_eMLy"
   },
   "source": [
    "## ***`MARCO TEORICO`***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Redes neuronales recurrentes***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definidos los vectores de palabras como entrada, vamos a construir la arquitectura de red para el análisis de los textos de interés. Un aspecto relevante de los datos de en el Procesamiento del lenguaje natural (PNL) es que tienen un aspecto temporal. Cada palabra de una oración depende en gran medida de lo que vino antes y después. Para tener en cuenta esta dependencia, se implementa una red neuronal recurrente.\n",
    "\n",
    "Es importante como ya se mencionó que en las redes neuronales recurrentes cada palabra en una secuencia de entrada, se asociará con un paso de tiempo específico. En efecto, el número de pasos de tiempo será igual a la longitud máxima de la secuencia.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/SentimentAnalysis18-877adcb2069f5d510b3bec36c50f3969.png\" width=\"700\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Estructura de la secuencia de palabras</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Asociado con cada paso de tiempo también hay un nuevo componente llamado el vector de estado oculto $h_t$ . Desde un nivel alto, este vector busca encapsular y resumir toda la información que se vio en los pasos de tiempo anteriores. Así como $x_t$ es un vector que encapsula toda la información de una palabra específica, $h_t$ es un vector que resume la información de los pasos de tiempos anteriores.\n",
    "\n",
    "El estado oculto es una función tanto del vector de palabra actual como del vector de estado oculto en el paso de tiempo anterior. La sigma $\\sigma$ indica que la suma de los dos términos se someterá a una función de activación (normalmente una sigmoide o tangente hiperbólica).\n",
    "\n",
    "$$h_t = \\sigma(W^h h_{t-1} + W^x x_t)$$\n",
    "\n",
    "Los términos de  $W$ en la formulación anterior representan matrices de peso. Si observa de cerca los superíndices, verá que hay una matriz de peso $W^x$ que vamos a multiplicar con nuestra entrada, y hay una matriz de peso recurrente $W^h$ que se multiplica con el vector de estado oculto en el paso de tiempo anterior. $W^x$ es una matriz que permanece igual en todos los pasos de tiempo, y la matriz de peso $W^x$ es diferente para cada entrada.\n",
    "\n",
    "Las matrices de peso se actualizan mediante un proceso de optimización denominado retropropagación en el tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Unidades de memoria a corto plazo (LSTM)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las unidades de memoria a corto plazo son módulos que puede colocar dentro de una red neuronal recurrente que aseguran, que el vector de estado oculto $h$ pueda encapsular información sobre dependencias a largo plazo en el texto. La formulación de $h$ en las RNN tradicionales es relativamente simple. Este enfoque no podrá conectar de manera efectiva la información que está separada por más de un par de pasos de tiempo. Podemos ilustrar esta idea de manejar dependencias a largo plazo a través de un ejemplo en el campo. La función de los modelos de respuesta a preguntas es tomar un pasaje del texto y responder una pregunta sobre su contenido.\n",
    "\n",
    "Mirando las unidades LSTM desde un punto de vista más técnico, las unidades toman el vector de palabra actual $x_t$ y dan salida al vector de estado oculto $h_t$. En estas unidades, la formulación de $h_t$ será un poco más compleja que la de un RNN típico. El cálculo se divide en 4 componentes, una puerta de entrada, una puerta de olvido, una puerta de salida y un nuevo contenedor de memoria, como se ilustra a continuación:\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/SentimentAnalysis10-5355aeffa295ce537c5a5b3a1cd7034b.png\" width=\"500\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Estructura de una red LSTM</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Cada puerta tomará $x_t$ y $h_{t-1}$ como entradas y realizará algunos cálculos en ellas para obtener estados intermedios. Cada estado intermedio se alimenta a diferentes tuberías y, finalmente, la información se agrega para formar $h_t$. En aras de la simplicidad, no entraremos en las formulaciones específicas para cada puerta, pero vale la pena señalar que cada una de estas puertas se puede considerar como módulos diferentes dentro del LSTM, cada uno con funciones diferentes. La puerta de entrada determina cuánto énfasis poner en cada una de las entradas, la puerta de olvido determina la información que desecharemos y la puerta de salida determina la $h_t$ final basado en los estados intermedios.\n",
    "\n",
    "Para profundizar en este tema se recomienda explorar en [Oreilly para el análisis de sentimientos](https://www.oreilly.com/content/perform-sentiment-analysis-with-lstms-using-tensorflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Implementación del modelo Glove`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe es un algoritmo de aprendizaje no supervisado para obtener representaciones vectoriales de palabras. El entrenamiento se realiza en estadísticas globales de co-ocurrencia palabra-palabra agregadas de un corpus, y las representaciones resultantes muestran interesantes subestructuras lineales del espacio vectorial de palabras.\n",
    "\n",
    "El modelo GloVe se entrena en las entradas distintas de cero de una matriz global de co-ocurrencia palabra-palabra, que tabula la frecuencia con la que las palabras coexisten entre sí en un corpus dado. Completar esta matriz requiere una sola pasada a través de todo el corpus para recopilar las estadísticas. Para grandes corpus, este pase puede ser computacionalmente costoso, pero es un costo inicial único. Las iteraciones de entrenamiento posteriores son mucho más rápidas porque el número de entradas de matriz distintas de cero suele ser mucho menor que el número total de palabras en el corpus.\n",
    "\n",
    "Las herramientas proporcionadas en este paquete automatizan la recopilación y preparación de estadísticas de co-ocurrencia para ingresar al modelo. El código de entrenamiento básico está separado de estos pasos de preprocesamiento y se puede ejecutar de forma independiente.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/3000/1*UNtsSilztKXjLG99VXxSQw.png\" width=\"700\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Estructura del modelo Glove</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "El objetivo de entrenamiento de GloVe es aprender los vectores de palabras de manera que su producto escalar sea igual al logaritmo de la probabilidad de co-ocurrencia de las palabras. Debido al hecho de que el logaritmo de una razón es igual a la diferencia de logaritmos, este objetivo asocia (el logaritmo de) razones de probabilidades de co-ocurrencia con diferencias vectoriales en el espacio vectorial de palabras. Debido a que estas proporciones pueden codificar alguna forma de significado, esta información también se codifica como diferencias vectoriales.\n",
    "\n",
    "Para extender la información se puede consultar en el siguiente link [Modelo Glove](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7YdfVEigWOl"
   },
   "source": [
    "## ***`Lenguaje Natural`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CA1iDwntgpPn"
   },
   "source": [
    "## ***`Definición y ejemplos mas usados para PLN (Procesamiento de Lenguaje Natural)`***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEIRyCeQWKJC"
   },
   "source": [
    "Los seres humanos humanos nos comunicamos a través de Lenguaje Natural (Ingles, Español, francés), lo anterior por medio de texto, voz y signos, actividades que actualmente ya procesa una computadora con programas como SIRI ó ALEXA, sin embargo surgió un gran reto y fue que los ordenadores pudiesen comunicarse con el hombre por medio del lenguaje natural, allí nació la Programación de Lenguaje Natural (PLN).\n",
    "El PLN, lo podemos definir como un campo de la inteligencia artificial que se encarga de investigar una forma efectiva de comunicar las máquinas con las personas a través del análisis grandes bases de datos estructuradas o no estructuradas de forma consistente e imparcial, sin fatigarse.\n",
    "\n",
    "Las computadoras solo entienden bytes y dígitos, los cuales son codificados por medio de programas como Python, se escriben una serie de reglas definiendo patrones computacionales y probabilidades de aparecer en un contexto determinado sin la exclusividad de cumplir con reglas gramaticales.\n",
    "\n",
    "Un ejemplo de un programa normal y un programa de Lenguaje Natural es: un programa como un Reconocimiento de voz y un PLN, el primero no necesita un análisis semántico o pragmático, cosa que si sucedería con sistema conversacional, el cual requiere entender lo que se le dice, procesarlo y entregar una respuesta con suficiente dominio temático. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI25Fwx-gbRl"
   },
   "source": [
    "**LOS COMPONENTES DEL PLN**\n",
    "1. Análisis morfológico: Se basa en análisis de una serie de palabras extrayendo rasgos flexivos, entre otros.\n",
    "2. Análisis sintáctico: Identifica la estructura de las oraciones usando la lógica y la estadística.\n",
    "3. Análisis semántico: Interpreta oraciones eliminando previamente las ambigüedades mofosintácticas.\n",
    "4. Análisis pragmático: Realiza el análisis de lenguaje figurado como metáforas o ironía.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_g4j9BSuQ-P"
   },
   "source": [
    "## ***`Modelos según Metodología`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiTtgPW_uRW0"
   },
   "source": [
    "**Modelo Booleano:**\n",
    "Se basa en Lógica Booleana y teoría de conjuntos.\n",
    "\n",
    "**Modelo Scoring:**\n",
    "Tiene como base la relevancia que se le de a determinados terminos, para realizar esta acción se otorgan score a los diferente terminos, entre los mas usados estan (Weighted Zone Scoring, Frecuencia de Término y Frecuencia Inversa de Documento).\n",
    "\n",
    "**Modelo de espacio vectorial:**\n",
    "Basicamente entiende cada documento como un conjunto de terminos que termina siendo asociado a un vector.\n",
    "\n",
    "**Preprocesado de Datos:** \n",
    "Esté será el modelo a usarse en el proyecto, los documentos son divididos en tokens, a continuación se identifican los terminos importantes y neutrales, posteriormente se aplican tecnicas de lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV2q9CxBgih5"
   },
   "source": [
    "## ***`Aplicaciones:`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYkRDAkZgHwh"
   },
   "source": [
    "A nivel general se puede utilizar buscando patrones para detectar o resolver delitos, clasificar temas que permitan descubrir tendencias, descubrir sentimientos, entre otros, a continuación veremos algunas de sus aplicaciones.\n",
    "\n",
    "\n",
    "* Sistemas conversaciones. (Interacción entre humanos y máquina por medio de voz “conversación”).\n",
    "\n",
    "* Recuperación y extracción  de información. Sus inicios fueron los buscadores de internet, teniendo en cuenta que con la llegada de esté no era fácil acceder a la cantidad de información publicada.\n",
    "\n",
    "* Etiquetado morfológico, sintáctico y semántico, los cuales se dividen en: \n",
    "1.   Respuestas automáticas a preguntas: Un ejemplo de ello es Watson de IBM que encuentra rápidamente la relación entre preguntas y respuestas utilizando gran cantidad de Bases de Datos y múltiples niveles de conocimiento.\n",
    "2.   Análisis de sentimientos de los textos: es la identificación del estado de ánimo en grandes cantidades de texto, mayormente usado en redes sociales permitiendo identificar y clasificar tipos de usuarios, sus intereses, su relación entre ellos. ***(será el que se aplicará en nuestro proyecto)***\n",
    " \n",
    "* Realización de resúmenes de textos automáticos:\n",
    "\n",
    "* Chatbots: Son sistemas capaces de tener una conversación coherente con una persona acerca de un tema específico.\n",
    "\n",
    "* Clasificación de documentos por categorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRsdVc_CgmJt"
   },
   "source": [
    "## ***`Analisis de Sentimientos`***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDEBIcj3yg_W"
   },
   "source": [
    "Es una aplicación de la Inteligencia Artificial que tiene como fin analizar textos cortos como calificaciones o comentarios realizados en Facebook, Twiter, etc. Este concepto nacio con el posicionamiento que tuvieron las redes sociales durante el año 2000, en esté momento las empresas vieron el gran potencial que tenian los comentarios y opiniones que publicaban los usuarios, estos textos en un corto tiempo se volvieron demasiados y algo dificiles de analizar, a partir de esta necesidad surgieron algoritmos que permitieron procesar dicha información.\n",
    "\n",
    "El objetivo de esté analisis es evaluar la calificación de un servicio o producto a partir de los comentarios realizados y definiendo el tono del escritor, esté se puede clasificar como Positivo, negativo o Neutro.\n",
    "\n",
    "Para realizar esta tarea se crean listados de palabras positivas y negativas, lo cual permite en el analisis de los textos evaluar si tiene mayor cantidad de palabras positivas o negativas. (Esta es la tecnica a aplicar en el proyecto trabajado)\n",
    "\n",
    "Es importante mencionar que existen metodos automaticos, estos se basan en tecnicas de aprendizaje automatico para ello es necesario tener una gran cantidad de ejemplos los cuales se le entregan a la computadora, de tal forma que pueda realizar un aprendizaje y a partir de allí entender patrones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E75i05YXdat-"
   },
   "source": [
    "PASOS:\n",
    "1. Extracción de la Información. (Web Scraping) y posterior limpieza de la  información en caso de que tenga datos como emojis.\n",
    "2. Procesamiento de los datos. Esto se hace con la librería de python TextBlob, la cual permite estimar el valor del sentimiento de los twits con un rango de polaridad de -1 a 1.\n",
    "3. Despliegue. Para realizar esta actividad se hace uso de la libreria librerías matplolib para los gráficos y wordcloud para crear una nube con los textos suministrados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qdrqghwXlrH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwbuBLfwgZMe"
   },
   "source": [
    "REFERENCIA BIBLIOGRAFICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPLFKTqJgb9H"
   },
   "source": [
    "**Historia Amazon.** https://historia-biografia.com/historia-de-amazon/\n",
    "\n",
    "\n",
    "**Documento comparativo entre diferentes servicios de streaming.** https://idus.us.es/bitstream/handle/11441/87550/Estudio_de_las_plataformas_de_streaming.pdf?sequence=1&isAllowed=y\n",
    "\n",
    "**Dive into Deep learning.**\n",
    "https://d2l.ai/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUhNAgj7hdIu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOutVBvIyHBeXR08m47da+i",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "04042021 Proyecto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
